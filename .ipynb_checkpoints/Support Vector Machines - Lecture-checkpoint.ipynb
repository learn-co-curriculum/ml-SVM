{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind SVM is that we perform classification by finding the line or (in higher dimenssions) \"hyperplane\" that differentiates begtween classes very well.\n",
    "\n",
    "We'll illustrate the idea with an easy example.\n",
    "\n",
    "Imagine we have a data set containing 2 classes and 2 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](SVM_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find a hyperplane or \"decision boundary\" that divides one class from the other. Which one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](SVM_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be a good line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](SVM_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But these other ones too... How to chose the best one? Either one of these three lines (or, using generalized language \"hyperplanes\") does equally well at classifying. The optimization objective in SVMs however is to **maximize the margin**. So what does this mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](SVM_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The margin is defined as the distance between the separating line (hyperplane) and the training set cases that are closest to this hyperplane, which are the so-called \"support vectors\". The suport vectors in this particular case are highlighted in the image below. As you can see, the max margin hyperplane is right in the middle between the two lines defined by the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](SVM_fin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Maximal margin classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would you bother maximizing the margins? Don't these other hyperplanes discriminate equally well? Remember that you are fitting the hyperplane on your training data. Imagine you start looking at your test data, which will slightly differ from your training data.\n",
    "\n",
    "Assuming your test set is big enough and randomly drawn from your entire data set, you might end up with a test case as shown on the image below. This test case diverts a little bit from the training set cases observed earlier. Where the max margin classifier would classify this test set case correctly, if you would have chosen the hyperplane closer to the right, this test case would have been classified incorrectly. Of course this is just one example, and other test cases will end up in a different spot, but the purpose of chosing the max margin classifier is to minimize the generalization error when applying the model to test set data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](SVM_test2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the margin maximization, let's take a closer look at the mathematical formulation of the hyperplanes defined by the support vectors. Let's look at the image again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](SVM_fin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some terminology: The lines defined by the support vectors are the negative (to the left) and the positive (to the right) hyperplanes respectively.\n",
    "\n",
    "\n",
    "$$ w_o + w_Tx_{pos} =1$$\n",
    "\n",
    "$$ w_0 + w_Tx_{neg} =-1$$\n",
    "\n",
    "$$ w_T(x_{pos}-x_{neg}) = 2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to normalize it by the length of the vector $||w||$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ || w ||= \\sqrt{\\sum^m_{j-1}w_j^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, divide the former expression by $||w||$. The left side of resulting equation can be interpreted as the distance between the positive and negative hyperplane. This is the **margin** we want to maximize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\dfrac{w_T(x_{pos}-x_{neg})}{\\lVert w \\rVert} = \\dfrac{2}{\\lVert w \\rVert}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the SVM is then maximizing $\\dfrac{2}{\\lVert w \\rVert}$ while constraining that the samples are classified correctly. Mathematically,\n",
    "\n",
    "$ w_o + w_Tx^{(i)} \\geq 1$  if $y ^{(i)} = 1$\n",
    "\n",
    "$ w_o + w_Tx^{(i)} \\geq -1$  if $y ^{(i)} = -1$\n",
    "\n",
    "For $i= 1,\\ldots ,N$\n",
    "\n",
    "These equations basically say that all negative samples should fall on on the left side of the negative hyperplane, whereas all the positive samples should fall on the right of the positive hyperplane. This can also be written in one line as follows:\n",
    "\n",
    "$y ^{(i)} (w_o + w_Tx^{(i)} )\\geq 1$  for each $i$\n",
    "\n",
    "Note that maximizing $\\dfrac{2}{\\lVert w \\rVert}$ means we're minimizing $\\lVert w \\rVert$, or, as is done in practice because it seems to be easier to be minimized, $\\dfrac{1}{2}\\lVert w \\rVert^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Soft margin classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing slack variables $\\xi$. The idea for introducing slack variables is that the linear constraints need to be relaxed for data that are not linearly saparable, as not relaxing the constraints might lead to the algorithm that doesn't converge. \n",
    "\n",
    "\n",
    "$ w_o + w_Tx^{(i)} \\geq 1-\\xi^{(i)}$  if $y ^{(i)} = 1$\n",
    "\n",
    "$ w_o + w_Tx^{(i)} \\geq -1+\\xi^{(i)}$  if $y ^{(i)} = -1$\n",
    "\n",
    "For $i= 1,\\ldots ,N$\n",
    "\n",
    "\n",
    "The objective function is \n",
    "\n",
    " $$\\dfrac{1}{2}\\lVert w \\rVert^2+ C(\\sum_i \\xi^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're basically adding these slack variables in your objective function, making clear that you want to minimize the amount of slack you allow for. You can tune this with the C variable as well. C will define how much slack we're allowing.\n",
    "\n",
    "- A big value for C will lead to the picture on the left: Misclassifications are heavily punished, so we'll prioritize classifying correctly over having a big margin.\n",
    "- A small value for C will lead to the picture on the right: it is OK to have some misclassifications, we'd rather have a bigger margin overall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](SVM_C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. non-linear problems: the kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: how would you draw a max margin classifier here? We could do it easily by hand, but we cannot draw a **straight line**. Luckily there is a fairly easy solution, which is denoted by kernelizing your problem such that you can solve non-linear classification problems. Here, we'll int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](SVM_nonlin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind kernel methods to deal with linearly inseparable data is to create (nonlinear) combinations of the original features, and project them on a higher-dimensional space. As shown in the following  figure, we can then   separate the cases by transforingm a two-dimensional dataset onto a new three-dimensional feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](SVM_kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/support-vector-machines-for-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikibooks.org/wiki/Support_Vector_Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/auto_examples/index.html#support-vector-machines\n",
    "\n",
    "http://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "http://www.statsoft.com/Textbook/Support-Vector-Machines\n",
    "\n",
    "https://en.wikipedia.org/wiki/Kernel_method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hackeling p171"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raschka p 180\n",
    "\n",
    "Rashka and Mirjalili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
